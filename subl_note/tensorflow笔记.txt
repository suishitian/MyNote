tensorflow学习笔记

第三章 Tensorflow入门
3.1 Tensorflow计算模型--计算图
  1.张量：Tensor，可以被简单的理解为多维数组。
  2.计算图：Tensorflow中的每一个计算都是计算图上的一个点，而节点之间的边则描述了计算之间的依赖关系。
3.2 计算图的使用
  1.定义阶段：
  eg:
  import tensorflow as tf
  a = tf.constant([1.0,2.0],name="a")
  b = tf.constant([2.0,3.0],name="b")
  result = a+b
  以上代码都在一个计算图上。
  2.不同的计算图上的张量和运算不会共享
  3.与计算图相关的部分API：
    (1)tf.Graph()  ----  生成一个计算图
    eg:g1 = tf.Graph()  ----  生成一个计算图g1
       g1 = tf.Graph()  ----  生成另一个计算图g2
    (2)print(a.graph is tf.get_default_graph())  ----  a为代码前面的一个变量，这句话会print一个Boolean用来判断a是否在默认的计算图上
  4.计算图不仅可以用来隔离张量和计算，还提供了管理张量和计算的机制。
    (1)
      with tf.Session(graph = g1) as sess:  ----  with后面的所有操作都只发生在计算图g1上，其他计算图上的变量不会受到影响
      	tf.initialize_all_variables().run()
      	with tf.variable_scope("",reuse=True)
      		print(sess.run(tf.get_variable("v")))
  5.tf.Graph.device用来指定计算图运行计算的设备(CPU,GPU)
  eg:
    g = tf.Graph()
    with g.device('/gpu:0')
    	result = a+b
3.2  tensorflow数据模型--张量
  3.2.1 张量的概念
    1.张量可以简单的被理解为多维数组。0阶张量可以理解为一个n维数组。但是实际上张量只是tensorflow对运算结果的引用。张量中并没有真正保存数值，保存的是数字的计算过程。
    2.张量的创建：
      a = tf.constant([1.0,2.0],name="a")
      b = tf.constant([1.0,3.0],name="b")
      result = a+b  ----  其中result就是一个张量，此处没有真正计算a+b的值，只是保存了result的计算过程，只是一个引用
    3.张量的三个属性：
      print result
      //Tensor("add:0" ,shape = (2,), dtype=float32)  ----  输出结果
      (1)名字name：第一个属性名字是一个张量的唯一标识符，同样这个名字也代表着张量是如何计算出来的。
         名字规则是node:src_output：计算图上的的node节点上的第src_output个输出(add:0代表add节点上的第1(索引是0)个输出)
      (2)维度shape：描述了张量的维度
      (3)数据类型type：每一个张量会有一个唯一的类型。tensorflow会对所有参与运算的张量进行类型检查，不匹配就会报错
    4.张量的使用
      (1)使用张量记录中间结果：
        a = tf.constant([1.0,2.0],name="a")
      (2)使用张量记录计算过程
        result = a+b
        tf.Session().run(result)  ----  用这个来得到结果，其中result就是张量，储存的是计算过程
3.3 tensorflow运行模型--会话
  1.会话Session是用来执行定义好的运算，可以管理程序运行时的所有资源。
  2.使用会话的两种模式：
    (1)需要明确调用会话生成函数和显示的关闭会话函数：
      sess = tf.Session()  ----  生成会话
      sess.run(...)  ----执行某种操作
      sess.close()  ----  必须显示的关闭该会话
      可以通过with来确保程序一定会正确关闭会话
      with tf.Session() as sess:
      	  sess.run(...)  ----  不需要显示的调用sess.close()，上下文with退出时候会自动关闭和资源释放，只要将所有的计算放在with内部就行了
    (2)手动指定一个默认的会话，然后可以通过tf.Tensor.eval函数来进行计算某个张量的取值
      sess = tf.Session()
      with sess.as_default():
      	  print(result.eval())  ----  使用.eval()来计算张量的值，因为已经指定了默认的会话
    (3)自动将生成的会话注册为默认会话：
      sess = tf.InteractiveSession()  ----  省去指定为默认会话的过程
      print(result.eval())
      sess.close()
    (4)在创建会话的时候进行设置：
      config = tf.ConfigProto(allow_soft_placement = True, log_device_placement=True) 
      sess1 = tf.InteractiveSession(config=config)  ----  两种方式都可以使用config进行设置
      sess2 = tf.Session(config=config)
3.4 tensorflow实现神经网络
  3.4.2 前向传播算法
    1.前向传播算法需要三部分的信息：
      (1)神经网络的输入：就是从实体中提取的特征向量
      (2)神经网络的连接结构：神经元的节点
      (3)每个神经元之间的参数。
    2.神经网络的结构以及边上的权重，就可以求出神经网络的输出：
      [a]=xW  ----  x为神经网络输入，W为对应层的参数矩阵
      [y]=aW  ----  y为输出，a为上一步得到的中间层的结果，W为对应层参数矩阵
  3.4.3 神经网络与tensorflow变量
    1.变量的作用是保存和更新神经网络中的参数
    2.变量的生成
      (1)随机变量：
        weights = tf.Variable(tf.random_normal([2,3],stddev=2))  ----  [2,3]是维数，stddev是标准差，生成的是2*3的高斯分布的随机变量。
      (2)常数变量：
        biases = tf.Variable(tf.zeros([3]))  ----  生成一个初始值全部为0的长度为3的变量
      (3)通过其他变量来设置新变量：
        w2 = tf.Variable(weights.initialized_value()*2.0)  ----  使用weight的初始值的两倍作为w2的值
    3.变量的值在被使用之前，变量的初始化过程需要被明确的调用
    eg:
      w1 = tf.Variable(tf.random_normal([2,3],stddev=1,seed=1))
      .....
      sess.run(w1.initializer)  ----  显示调用w1的初始化
    4.初始化所有变量：
      init_op = tf.initialize_all_variables()  ----  返回所有的变量
      sess.run(init_op)  ----  对所有变量进行初始化，并且会自动处理变量之间的依赖关系
    5.变量是一种特殊的张量，tf.Variable其实就是一种运算，只不过输出是一个张量。
    6.变量的类型是不可变的，一个变量在构建之后，他的类型就不能再改变了。
  !3.4.4 通过tensorflow训练神经网络模型
    1.每次迭代开始之前，都要先选取一小部分训练数据，叫做batch。通过batch做出的预测与正确结果作比较，得出差距，并且利用反向传播算法更新相应的神经网络参数的取值。
    2.因为每次迭代需要大量的张量计算，为了避免计算图太大，可以使用placeholder。placeholder相当于占位，每次迭代将batch传入placeholder即可。placeholder的类型也是不能改变的。
    eg:
    w1 = tf.Variable(tf.random_normal([2,3],stddev = 1,seed=1))
    w2 = tf.Variable(tf.random_normal([3,1],stddev = 1,seed=1))

    x = tf.placeholder(tf.float32,shape=[3,2],name="input")  ----  用placeholder进行占位
    a = tf.matmul(x,w1)
    y = tf.matmul(a,w2)

    sess = tf.Session()
    init_op = tf.initialize_all_variables()
    sess.run(init_op)

    print(sess.run(y,feed_dict={x:[[0.7,0.9],[0.1,0.4],[0.5,0.8]]}))  ----  需要提供feed_dict来制定x的值。
    3.损失函数：刻画预测值与真实值的差距
      cross_entropy = -tf.reduce_mean(y_*tf.log(tf.clip_by_value(y,le-10,1.0)))  ----  交叉熵
      learning_rate = 0.001  ----  学习率
      train_step =\ tf.train.AdamOptimizer(learning_rate).minimize(cross_entroy)
  3.4.5 完整的神经网络程序
    1.完整的程序
    import tensorflow as tf
    from numpy.random import RandomState

    batch_size = 8  ----  设定batch值

    w1 = tf.Variable(tf.random_normal([2,3],stddev = 1,seed = 1))  ----  产生随机w参数值
    w2 = tf.Variable(tf.random_normal([3,1],stddev = 1,seed = 1))

    x = tf.placeholder(tf.float32,shape=[None,2],name="x_input")  ----  设定placeholder
    y_ = tf.placeholder(tf.float32,shape=[None,1],name = "y_input")

    a = tf.matmul(x,w1)  ----  前向传播第一层
    y = tf.matmul(a,w2)  ----  前向传播第二层

    cross_entropy = -tf.reduce_mean(y_*tf.log(tf.clip_by_value(y,1e-10,1.0)))  ----  定义反向传播计算交叉熵的算法
    train_step = tf.train.AdamOptimizer(0.001).minimize(cross_entropy)  ----  定义优化参数的算法

    rdm = RandomState(1)
    dataset_size = 128

    X = rdm.rand(dataset_size,2)  ----  随机生成输入数据
    Y = [[int(x1+x2<1)] for (x1,x2) in X]  ----  正确数据

    with tf.Session() as sess:  ----  开启会话
        init_op = tf.initialize_all_variables()  ----  初始化所有变量
        sess.run(init_op)
        print sess.run(w1)  ----  打印生成的随机参数
        print sess.run(w2)

        STEPS = 5000  ----  迭代次数
        for i in range(STEPS):
            start = (i*batch_size)%dataset_size  ----  根据batch计算完整数据中的起始位置
            end = min(start+batch_size,dataset_size)  ----  结束位置

            sess.run(train_step,feed_dict={x:X[start:end],y_:Y[start:end]})  ----  进行训练
            if i%1000 == 0:  ----  每1000次打印交叉熵
                total_cross_entropy = sess.run(cross_entropy,feed_dict={x:X,y_:Y})
                print("After %d training step(s),cross entropy on all data is %g"%(i,total_cross_entropy))
        print sess.run(w1)  ----  跳出循环(训练完毕)之后，打印最终优化之后的两层参数
        print sess.run(w2)
    2.训练神经网络的过程
      (1)定义神经网络的结构和前向传播的输出结果
      (2)定义损失函数以及选择反向传播优化算法
      (3)生成会话(tf.Session())并且在训练数据上反复运行反向传播优化算法

第四章 深层神经网络
4.1 深度学习与深层神经网路
  4.1.2 激活函数实现去线性化
    1.加入偏置项
    2.每个节点的取值(输出)不再是单纯的加权和，而是在输出后面加上一个激活函数：
      (1)ReLU: f(x) = max(x,0)
      (2)sigmoid: f(x) = 1/(1+e^(-x))
      (3)tanh: f(x) = (1-e^(-2x))/(1+e^(-2x))
      (4)tensorflow中的应用：tf.nn.relu,tf.sigmoid,tf.tanh
      (5)自定义激活函数：
        a = tf.nn.relu(tf.matmul(x,w1)+biases1)
  4.1.3 多层网络解决异或运算
4.2 损失函数定义
  4.2.1 经典损失函数
    1.交叉熵
      1.分类问题一般是设置N个输出节点，每一个节点代表一个类别，哪个输出最高则表示分到那一类。期望的输出为[00010000]，就是某一项是0其他都是1
      2.交叉熵：用来评判一个输出向量与期望概率分布的距离(只能用于概率分布)，交叉熵越小代表预测值与真实值的距离越小。所以优化的目标就是让交叉熵尽量小
        H(p,q) = -∑p(x)logq(x)  ----  p是正确答案，q是预测值
      3.如果将分类问题中”一个样例属于某个类别“看成一个概率时间，那么训练数据的正确答案就符合一个概率分布。讲前向传播结果变成一个概率分布的办法就是使用softmax回归。
        (1)softmax是在最后的Output层前面的一个额外的物理层。
        (2)如果原始的输出为y1,y2,y3...,yn。则经过softmax之后的新的输出为：
          softmax(yi) = yi` = e^yi/(∑(j=1~n)e^yj)
        (3)经过处理之后的数据就变成一个概率分布，所以就能够通过交叉熵计算概率分布和真正答案之间的距离了。
        (4)tensorflow中的交叉熵：
          cross_entropy = -tf.reduce_mean(y_*tf.log(tf.clip_by_value(y,1e-10,1.0)))
            ----  y_为正确数据
            ----  tf.clip_by_value可以讲一个张量中的数值限制在一个范围之内，在这里为了避免log0，将其限制在无限接近0和1之间
            ----  tf.log对张量中所有元素一次求对数的功能
            ----  *为普通乘法，即对应元素相乘。矩阵乘法需要使用tf.matmul()
            ----  最后的recude_mean求一个batch中的交叉熵平均值
        (5)使用了softmax回归之后的交叉熵损失函数：
          cross_entropy = tf.nn.softmax_cross_entropy_with_logits(y,y_)  ----  y_正确数据，y预测值
    2.均方误差
      1.回归问题解决的是对数值的预测，这些问题需要的是一个实数，并且神经网络的输出节点往往只有一个输出节点，这个节点的输出值就是预测值。
      2.对于回归问题最常用的就是MSE均方误差：同样越小越好
        MSE(y,y') = (∑(i=1~n)(yi-yi')^2)/n  ----  y为正确答案，yi为正确答案中的一个值，y'是预测值，yi'是某个预测值
      3.tensorflow实现均方误差：
        mse = tf.reduce_mean(tf.square(y_ - y))
          ----  -为普通减法，就是对应元素相减
          ----  y_为正确值
  4.2.2 自定义损失函数 P78

4.3 神经网络优化算法
  1.反向传播算法：训练神经网络的核心算法，给出了一个高效的方式子所有参数上使用梯度下降算法。
  2.梯度下降算法：调整神经网络中参数的取值，主要用于优化单个参数的取值。(反向传播算法会优化梯度下降算法在所有参数中的计算效率)
    eg：J(thet) 是一个损失函数，找到一个参数thet让损失函数的值最小
  3.对于参数thet梯度为δ/δthet(J(thet))，其实就是一个偏导。还需要定义一个学习率n来定义每次参数更新的幅度。
    thet(n+1) = thet(n) - 学习率*偏导
  4.梯度下降算法不能保证会达到全局最优解，有可能得到的是局部最优解。所以参数的初始值非常重要(从哪里开始下降，是局部的坡，还是全局的坡)
  5.梯度下降算法另一个问题就是计算时间太长。为了减少优化时间，使用随机梯度下降算法：在每一轮迭代中，随机优化某一条训练数据上的损失函数。但是随之带来的问题就是，只优化一条数据并不意味着能够使全局损失函数更小，所以有时甚至无法达到局部最优解。
  6.为了综合梯度下降和随机梯度下降的优缺点，使用一个这种的办法，就是每次值计算一小部分数据的损失函数。这一小部分数据被称之为batch。
    (1)batch_size的选取不能太大也不能太小，太小就等于随机梯度下降，可能无法达到最优解，太大就会拖慢运行速度。

4.4 神经网络的进一步优化
  4.4.1 学习率的设置
    1.指数衰减法：
      decayed_learning_rate = learning_rate * decay_rate^(global_step/decay_steps)
        ----  decayed_learning_rate为每一轮优化实际使用的最终学习率
        ----  deay_rate为衰减系数
        ----  decay_step为衰减速度
        ----  global_step为全局的迭代轮数
        ----  还有一个staircase参数，如果选为true会在每完整的过完一边训练数据之后，学习率就减小一次。
    2.tensorflow来设置学习率：
      global_step = tf.Variable(0)
      learning_rate = tf.train.exponential_decay(0.1,global_step,100,0.96,staircase=True)
        ----  0.1 基础学习率
        ----  100 衰减速度(需要global减去这个东西)
        ----  0.96 衰减率(指数的底数)
      其中global_step会在.minimize中自动更新：
        learning_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,global_step=global_step)
          ----  真个函数就是梯度下降算法，输入参数为学习率
          ----  loss为需要优化的损失函数。可以为cross_entropy也可以是MSE
  4.4.2 过拟合问题
    1.对训练数据拟合非常好，对测试数据拟合极差。为了避免过拟合，需要使用正则化。
    2.正则化就是在原来的损失函数上，加上一个刻画复杂的指标。用来限制权重的大小，使得模型不能任意拟合训练数据中的随机噪音。
      L1正则化： R(w) = ||w||1 = ∑|wi|
      L2正则化： R(w) = ||w||^2 = ∑|wi^2|
    3.实际中使用正则化是L1L2一起使用：
      R(w) = ∑(a|wi|+(1-a)wi^2)
    4.tensorflow优化带L2正则化的损失函数：
      w = tf.Variable(tf.random_normal([2,1],stddev = 1,seed = 1))
      y = tf.matmul(x,w)

      loss = tf.reduce_mean(tf.square(y_ - y))+tf.contrib.layer.l2_regularizer(lambda)(w)
        ----  tf.contrib.layer.l2_regularizer输入的是正则化的权重就是a，返回一个函数，返回的函数带进去需要正则化的参数，会最终返回正则化之后的值

    5.使用collection来存储每一个参数的正则化：随着神经网络的复杂，可能有时候网络结构和计算损失函数的部分不在一起。
      P88
  4.4.3 滑动平均模型
    1.每一个变量都另外维护一个影子变量，影子变量的初始值就是相应变量的初始值，每次运行变量更新的时候影子变量的值就会更新为：
      shadow_variable = decay*shadow_variable+(1-delay)*variable
        ----  shadow_variable为影子变量
        ----  variable为待更新的变量
        ----  decay为衰减率
      decay决定了模型更新的速度，decay越大模型就越趋于稳定。实际应用中，一般将decay设定为非常接近1的数字，为了让模型在前期可以更新的更快
    2.ExponentialMovingAverage提供了num_updates参数来动态设置decay的大小：
      衰减率将会是：min{decay,(1+num_updates)/(10+nu_updates)}  ----  也就是decay和另一个参数中更小的东西
    3.tensowflow实现滑动平均模型
      P90
      step = tf.Variable(0,trainable=false)
      ema = ef.train.ExpoenetialMovingAverage(0.99,step)
      maintain_averages_op = ema.apply([v1])  ----  对v1数据进行滑动平均，即只要v1中的值发生变化，v1的影子变量也会变化
      ......
      print sess.run([v1,ema.average(v1)])  ----  之后想要获取v1的滑动平均变量的时候只需要使用ema.average()即可

第五章 MNIST数字识别问题
5.1 MNIST数据处理
5.2 Tensorflow训练神经网络
  5.2.1 TensorFLow训练神经网络
    import tensorflow as tf
    from tensorflow.examples.tutorials.mnist import input_data    

    #
    INPUT_NODE = 784  ----  输入节点个数
    OUTPUT_NODE = 10  ----  输出节点个数，代表了10种分类    

    LAYER_NODE = 500  ----  中间层的节点个数    

    BATCH_SIZE = 100  ----  batch_size    

    LEARNING_RATE_BASE = 0.8  ----  基础学习率
    LEARNING_RATE_DECAY = 0.99  ----  学习率的衰减率(指数的底数)
    REGULARIZATION_RATE = 0.0001  ----  正则化权重
    TRAINING_STEPS = 10000  ----  总的迭代次数
    MOVING_AVERAGE_DECAY = 0.99  ----  平均滑动衰减率
    #
    def inference(input_tensor,avg_class,weights1,biases1,weights2,biases2):  ----  定义了一个前向传播算法函数，直接从输入节点计算到输出节点的结果
        if avg_class == None:  ----  如果没有传入滑动平均类，即计算普通的变量更新
            layer1 = tf.nn.relu(tf.matmul(input_tensor,weights1)+biases1)  ----  第一层的结果(带偏置量)，并且激活函数为ReLU
            return tf.matmul(layer1,weights2)+biases2  ----  output的计算，这里没有使用Relu是因为在计算交叉熵的时候会自动使用softmax所以这里没有加入激活函数
        else:
            layer1 = tf.nn.relu(tf.matmul(input_tensor,avg_class.average(weights1))+avg_class.average(biases1))
            return tf.matmul(layer1,avg_class.average(weights2))+avg_class.average(biases2)
    #
    def train(mnist):  ----  定义了整个训练的过程
      #placeholder
      x = tf.placeholder(tf.float32,[None,INPUT_NODE],name="x-input")  ----  定义placeholder
      y_ = tf.placeholder(tf.float32,[None,OUTPUT_NODE],name="y-input")    

      weights1 = tf.Variable(tf.truncated_normal([INPUT_NODE,LAYER_NODE],stddev=0.1))  ----  随机生成第一层参数，以及偏置量
      biases1 = tf.Variable(tf.constant(0.1,shape=[LAYER_NODE]))    

      weights2 = tf.Variable(tf.truncated_normal([LAYER_NODE,OUTPUT_NODE],stddev=0.1))  ----  随机生成第二层参数，以及偏置量
      biases2 = tf.Variable(tf.constant(0.1,shape=[OUTPUT_NODE]))    

      global_step = tf.Variable(0,trainable=False)  ----  全局步数，会在.minimize()中自动+1
      #
      y = inference(x,None,weights1,biases1,weights2,biases2)  ----  计算前向传播的结果，也就是预测值
      #
      variable_average = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY,global_step)  ----  滑动平均
      variables_averages_op = variable_average.apply(tf.trainable_variables())  ----  对所有可训练的数据都进行滑动平均
      #
      average_y = inference(x,variable_average,weights1,biases1,weights2,biases2)  ----  输入了滑动平均类的前向传播结果
      #
      cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits = y,labels = tf.argmax(y_,1))  ----  交叉熵(因为结果是概率分布)，且自动加入了softmax层
      cross_entropy_mean = tf.reduce_mean(cross_entropy)    

      #
      regu = tf.contrib.layers.l2_regularizer(REGULARIZATION_RATE)  ----  L2正则化
      regu_total = regu(weights1) + regu(weights2)  ----  两层正则化的和
      #
      loss = cross_entropy_mean+regu_total  ----  完整的损失函数

      learning_rate = tf.train.exponential_decay(  ----  指数衰减的学习率
        LEARNING_RATE_BASE,
        global_step,
        mnist.train.num_examples/BATCH_SIZE,
        LEARNING_RATE_DECAY)
    #
      train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,global_step=global_step)  ----  优化函数，梯度下降
      with tf.control_dependencies([train_step,variables_averages_op]):
        train_op = tf.no_op(name='train')    

      correct_prediction = tf.equal(tf.argmax(average_y,1),tf.argmax(y_,1))
      correct_prediction_noa = tf.equal(tf.argmax(y,1),tf.argmax(y_,1))
      accuary = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))
      accuary_noa = tf.reduce_mean(tf.cast(correct_prediction_noa,tf.float32))
      
      with tf.Session() as sess :
        tf.initialize_all_variables().run()
        validate_feed = {x:mnist.validation.images,
              y_:mnist.validation.labels}
        test_feed = {x:mnist.test.images, y_:mnist.test.labels}
            

        for i in range(TRAINING_STEPS):  ----  训练过程
          if i%1000==0:
            validate_acc= sess.run(accuary,feed_dict = validate_feed)
            print("After %d training steps, validation accuracy using average model is %g"
              %(i,validate_acc))
          xs,ys = mnist.train.next_batch(BATCH_SIZE)
          sess.run(train_op,feed_dict={x:xs,y_:ys})
        
        test_acc = sess.run(accuary,feed_dict=test_feed)
        print("After %d training steps, test accuracy using average model is %g"
            %(TRAINING_STEPS,test_acc))
      print "over"

    def main(atgv = None):
      mnist = input_data.read_data_sets("tmp/data",one_hot=True)
      train(mnist)    

    if __name__ == '__main__':
      tf.app.run();
  5.2.2 使用验证数据集判断模型效果
    交叉验证：讲一个数据分为k分，其中的一份用作测试数据，剩余的k-1份用作训练。然后重复k次，让每一份k得到验证，就是交叉验证
  5.2.3 不同模型的效果比较
    P102

5.3 变量管理
  1.tensorflow中提供了通过变量名称来创建或者获取一个变量的机制。通过这个机制可以在不同函数中直接通过获取变量的名字来使用变量。而不需要将变量通过参数的形式到处传递。主要是通过tf.get_variable和tf.variable_scope实现的
  2.用tf.get_variable和tf.Variable创建变量
    v = tf.get_variable("v",shape=[1],initializer=tf.constant_initializer(1.0))  ----  这两句几乎没有区别
    v = tf.Variable(tf.constant(1.0,shape=[1],name="v"))
  3.用tf.get_variable生成变量时候，如果变量名已经定义过，就会报错。想用get_variable获取变量，则需要使用tf.variable_scope生成一个上下文
    with tf.Variable_scope("foo"):
      v = tf.get_variable("v",[1],initializer = tf.constant_initializer(1.0))
    with tf.Variable_scope("foo"):
      //v = tf.get_variable("v",1)  ----  这一句会报错，因为上面已经在foo命名空间中间定义过v了，重复定义会报错
    with tf.Variable_scope("foo",reuse=True):  ----  如果在生成命名空间的时候加上reuse=True，则会直接获取已经声明了的变量
      v1 = tf.get_variable("v",[1])  ----  获取已经声明过的v
      print v==v1  ----  输出为true
    with tf.variable_scope("bar",reuse=True):  ----  将参数设置为reuse=true时候，则只能获取已经声明过的变量
      v = tf.get_variable("v",1)  ----  报错，因为v在bar中没有声明过
  4.scope可以嵌套：内层如果不指定，则reuse跟随外层
    with tf.bariable_scope("root"):  ----  最外层不指定为false
      with tf.variable_scope("foo",reuse=True)
        ...
        with tf.variable_scope("bar")  ----  这里的reuse为True,不指定就跟随外层
      ....  ----  回到这里之后，又为false
  5.使用scope管理变量：在命名空间中创建的变量都会带上这个命名空间名作为前缀
    v1 = tf.get_variable("v",[1])
    print v1.name  ----  输出v:0

    with tf.variable_scope("foo"):
      v2 = tf.get_variable("v",[1])
      print v2.name  ----  输出foo/v:0

    with tf.variable_scope("foo"):
      with tf.variable_scope("bar"):
        v3 = tf.get_variable("v",[1])
        print v3.name  ----  输出foo/bar/v:0
    with tf.variable_scope("",reuse = True)
      v4 = tf.get_variable("foo/bar/v",[1])  ----  也可以直接制定命名空间
    print v4==v3  ----  True
  6.使用命名空间改进前向传播函数inference:

    def inference(input_tensor,reuse=False):  ----  第一次调用使用false，用来生成变量，计算的时候使用True用来直接获取变量
      with tf.variable_scope('layer1',reuse=reuse):
          weights = tf.get_variable("weights",[INPUT_NODE,LAYER1_NODE],initializer=tf.truncated_normal_initializer(stddev=))
          biases = tf.get_variable("biases",[LAYER1_NODE]，initializer=tf.constant_initializer(0.0))
          layer1 = tf.nn.relu(tf.matmul(input_tensor,weights)+biases)
      with tf.variable_scope('layer2',reuse=reuse):
          weights = tf.get_variable("weights",[LAYER1_NODE,OUTPUT_NODE],initializer=tf.truncated_normal_initializer(stddev=))
          biases = tf.get_variable("biases",[OUTPUT_NODE]，initializer=tf.constant_initializer(0.0))
          layer2 = tf.matmul(input_tensor,weights)+biases
      return layer

5.4 TensorFlow模型持久化：
  5.4.1 持久化代码实现
    1.使用tf.train.Saver()来保存模型
      import tensorflow as tf
        v1 = tf.Variable(tf.constant(1.0,shape=[1],name="v1"))
        v2 = tf.Variable(tf.constant(2.0,shape=[1],name="v2"))
        result = v1+v2      
        init_op = tf.initialize_all_variables()       
        saver = tf.train.Saver()  ----  单纯为了获取一个Saver对象
        with tf.Session() as sess:
            sess.run(init_op)
            saver.save(sess,"model.ckpt")  ----  要注意，保存的时候，会话也作为输入参数
    2.Saver会在目录下生成三个文件：
      (1)model.ckpt.meta:保存了tensorflow的计算图的结构
      (2)model.ckpt:保存了程序中每一个变量的取值
      (3)checkpoint:后面会详细讲解
    3.使用tf.train.Saver()来恢复模型：
      import tensorflow as tf
      saver = tf.train.import_meta_graph("model.ckpt.meta")  ----  恢复数据
      with tf.Session() as sess:
        saver.restore(sess,"model.ckpt")  ----  恢复计算图
        print sess.run(tf.get_default_graph().get_tensor_by_name("add:0"))  ----  计算计算图中的某个节点的输出值

    4.使用Saver()加载部分数据,tensorflow通过字典将模型保存时的变量名和需要加载的变量联系起来：
      Saver = tf.train.Saver({"v1":new_v1,"v2":new_v2})  ----  不仅可以加载部分数据，还可以对数据进行重命名
    5.使用Saver()保存EMA数据：
        import tensorflow as tf
          v = tf.Variable(0,dtype = tf.float32,name="v")          

          for variable in tf.all_variables():
              print variable.name
              
          ema = tf.train.ExponentialMovingAverage(0.99)
          maintain_averages_op = ema.apply(tf.all_variables())
          saver = tf.train.Saver()
          for variable in tf.all_variables():
              print variable.name
              
          with tf.Session() as sess:
              init_op = tf.initialize_all_variables()
              sess.run(init_op)         

              sess.run(tf.assign(v,10))
              sess.run(maintain_averages_op)          

              saver.save(sess,"model1.ckpt")  ----  讲数据保存在model1模型中
              print sess.run([v,ema.average(v)])
    6.使用Saver()提取EMA数据：
      import tensorflow as tf
      v = tf.Variable(0,dtype = tf.float32,name="v")
      ema = tf.train.ExponentialMovingAverage(0.99)
      print ema.variables_to_restore()        

      saver = tf.train.Saver(ema.variables_to_restore())  ----  这句将EMA类中的一个方法作为输入，则提取的数据则为EMA数据
      #saver = tf.train.Saver()  ----  如果不讲EMA的那个方法作为输入，则这句会正常输入数据的真实值

      with tf.Session() as sess:
          saver.restore(sess,"model1.ckpt")
          print sess.run(v)
    7.可以将生成的三个文件合成为一个，直接讲训练好的变量存储为常量，在迁移学习的时候会用到，需要使用convert_variables_to_contants
      import tensorflow as tf
      from tensorflow.python.framework import graph_util      

      v1 = tf.Variable(tf.constant([1.0],shape=[1]),name = "v1")
      v2 = tf.Variable(tf.constant([2.0],shape=[1]),name = "v2")
      result = v1+v2      

      init_op = tf.initialize_all_variables()
      with tf.Session() as sess:
          sess.run(init_op)
          graph_def = tf.get_default_graph().as_graph_def()  ----  到处计算图GraphDef部分，只需要这一部分就可以完成从输入层到输出层的计算  

          output_graph_def = graph_util.convert_variables_to_constants(sess,graph_def,['add'])  ----  ['add']为节点的名称(节点的名称没有:0)
          with tf.gfile.GFile("combined_model.pb","wb") as f:
              f.write(output_graph_def.SerializeToString())  ----  写入
      提取：
      import tensorflow as tf
      from tensorflow.python.platform import gfile
      with tf.Session() as sess:
          model_filename = "combined_model.pb"
          with gfile.FastGFile(model_filename,'rb') as f:  ----  读取文件
              graph_def = tf.GraphDef()
              graph_def.ParseFromString(f.read())      

          result = tf.import_graph_def(graph_def,return_elements=["add:0"])  ----  读取图中的计算，这里使用了["add:0"]是因为这里需要的是张量，(张量的名称有:0)
          print sess.run(result)
  5.4.2 持久化原理及数据格式
    1.元图MetaGraphDef,.meta为后缀名:
      (1)MetaGraphDef格式：
        message MetaGraphDef{
          MetaInfoDef meta_info_def = 1;  ----  记录了元数据以及用到的所有计算方法的信息P117
          GraphDef graph_def = 2;  ----  主要记录了图上的节点信息，只关注运算的连接结构P119
          SaverDef saver_def = 3;  ----  持久化模型需要用到的一些参数，比如保存文件的文件名，保存操作，加载操作等
          map<string,CollectionDef> collection_def = 4;
          map<string,SignatureDef> signature_def = 5;
        }
      (2).ckpt文件：保存了所有变量的取值，这个文件是通过SSTable格式储存的(其实就是一个key,value列表)
      (3)checkpoint：由tf.train.Saver()类自行维护，持久化了所有tensorflow模型文件的文件名，当某个模型文件被删除时候，这个文件所对应的文件名也会从checkpoint中删除
    2.使用tf.train.NewCheckpointReader类读取.cpkt中保存的变量
      import tensorflow as tf
        reader = tf.train.NewCheckpointReader("model.ckpt")
        all_variables = reader.get_variable_to_shape_map()
        for variable_name in all_variables:
            print variable_name,all_variables[variable_name]        

        print "value of v1 is", reader.get_tensor("v")  ----  使用reader提取制定变量名的变量
5.5 TensorFlow最佳时间样例程序
  将原来的mnist训练分成三个文件：
    mnist_inference.py  ----  定义了前向传播的过程以及神经网络中的参数
    mnist_train.py  ----  定义神经网络训练的过程
    mnist_eval.py  ----  定义了测试过程
  (1)mnist_inference:
    import tensorflow as tf
      INPUT_NODE = 784
      OUTPUT_NODE = 10
      LAYER1_NODE = 500     

      def get_weight_variable(shape,regularizer):
          weights = tf.get_variable("weights",shape
                                    ,initializer=tf.truncated_normal_initializer(stddev=0.1))
          if regularizer != None:
              tf.add_to_collection('losses',regularizer(weights))
          return weights      

      def inference(input_tensor,regularizer):
          with tf.variable_scope('layer1'):
              weights = get_weight_variable([INPUT_NODE,LAYER1_NODE],regularizer)
              biases = tf.get_variable("biases",[LAYER1_NODE]
                                       ,initializer=tf.constant_initializer(0.0))
              layer1 = tf.nn.relu(tf.matmul(input_tensor,weights)+biases)     

          with tf.variable_scope('layer2'):
              weights = get_weight_variable([LAYER1_NODE,OUTPUT_NODE],regularizer)
              biases = tf.get_variable("biases",[OUTPUT_NODE]
                                       ,initializer=tf.constant_initializer(0.0))
              layer2 = tf.matmul(layer1,weights)+biases
          return layer2
  (2)mnist_train:
    import tensorflow as tf
    import os
    from tensorflow.examples.tutorials.mnist import input_data    

    import mnist_inference    

    BATCH_SIZE = 100
    LEARNING_RATE_BASE = 0.8
    LEARNING_RATE_DECAY = 0.99
    REGULARAZTION_RATE = 0.0001
    TRAIN_STEPS = 10000
    MOVING_AVERAGE_DECAY = 0.99    

    MODEL_SAVE_PATH = "/data"
    MODEL_NAME = "model.ckpt"
    def train(mnist):
        print "train start"
        x = tf.placeholder(tf.float32,[None,mnist_inference.INPUT_NODE],name="x-input")
        y_ = tf.placeholder(tf.float32,[None,mnist_inference.OUTPUT_NODE],name="y-input")    

        regularizer = tf.contrib.layers.l2_regularizer(REGULARAZTION_RATE)
        y = mnist_inference.inference(x,regularizer)
        global_step = tf.Variable(0,trainable=False)    

        ema = tf.train.ExponentialMovingAverage(MOVING_AVERAGE_DECAY,global_step)
        ema_op = ema.apply(tf.trainable_variables())
        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y,labels=tf.argmax(y_,1))
        cross_entropy_mean = tf.reduce_mean(cross_entropy)
        loss = cross_entropy_mean + tf.add_n(tf.get_collection('losses'))
        learning_rate = tf.train.exponential_decay(
            LEARNING_RATE_BASE,
            global_step,
            mnist.train.num_examples/BATCH_SIZE,
            LEARNING_RATE_DECAY)
        train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss,global_step=global_step)
        with tf.control_dependencies([train_step,ema_op]):
            train_op = tf.no_op(name='train')
        saver = tf.train.Saver()
        with tf.Session() as sess:
            sess.run(tf.global_variables_initializer())
            print "before for loop"
            for i in range(TRAIN_STEPS):
                xs,ys = mnist.train.next_batch(BATCH_SIZE)
                _,loss_value,step = sess.run([train_op,loss,global_step],feed_dict={x:xs,y_:ys})
                
                if i%1000==0:
                    print("After %d training step, loss is %g"%(step,loss_value))
                    saver.save(sess,os.path.join(MODEL_SAVE_PATH,MODEL_NAME),global_step = global_step)
        #print "over"
    def main(argv=None):
        mnist=input_data.read_data_sets("/tmp/data",one_hot=True)
        train(mnist)    

    if __name__ == '__main__':
        tf.app.run()
  (3)mnist_eval:
    import time
    import tensorflow as tf
    from tensorflow.examples.tutorials.mnist import input_data    

    import mnist_inference
    import mnist_train    

    EVAL_INTERVAL_SECS = 10    

    def evaluate(mnist):
        with tf.Graph().as_default() as g:
            x=tf.placeholder(tf.float32,[None,mnist_inference.INPUT_NODE],name="x_input")
            y_=tf.placeholder(tf.float32,[None,mnist_inference.OUTPUT_NODE],name="y_input")
            validate_feed = {x:mnist.validation.images,y_:mnist.validation.labels}    

            y=mnist_inference.inference(x,None)
            correct_prediction = tf.equal(tf.argmax(y,1),tf.argmax(y_,1))
            acc = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))    

            ema = tf.train.ExponentialMovingAverage(mnist_train.MOVING_AVERAGE_DECAY)
            variable_restore = ema.variables_to_restore()
            saver = tf.train.Saver(variable_restore)    

            while True:
                with tf.Session() as sess:
                    ckpt = tf.train.get_checkpoint_state(mnist_train.MODEL_SAVE_PATH)
                    if ckpt and ckpt.model_checkpoint_path:
                        saver.restore(sess,ckpt.model_checkpoint_path)    

                        global_step = ckpt.model_checkpoint_path.split('/')[-1].split('-')[-1]
                        acc_score = sess.run(acc,feed_dict=validate_feed)
                        print("After %s training step,validation acc = %g"%(global_step,acc_score))
                    else:
                        print("No checkpoint file")
                        return
                    time.sleep(EVAL_INTERVAL_SECS)
    def main(argv=None):
        mnist=input_data.read_data_sets("/tmp/data",one_hot = True)
        evaluate(mnist)
    if __name__ == '__main__':
        tf.app.run()

第六章 图像识别与卷积神经安利过
6.1 图像识别问题简介以及经典数据集
6.2 卷积神经网络简介 
  1.卷积神经网络的结构：
    (1)输入层：为一个三维矩阵，长宽为图片的大小，深度为色彩通道(根据不同节点的颜色取值)。从输入层开始，卷积神经网络通过不同的神经网络结构讲上一层的三维矩阵转化为下一层的三维矩阵，知道最后的全连接层
    (2)卷积层：卷积层中每一个节点的输入只是上一层神经网络的一小块，卷积层试图将神经网络中的每一个小块进行更加深入的分析从而得到抽象成都更高的特征。所以一般经过卷积层处理过的节点矩阵会变得更深。
    (3)池化层：池化层不会改变神经网络中的三维矩阵的深度，但是可以缩小矩阵的大小(可以理解为降低图片的分辨率)。这样可以进一步缩小最后全连接层中的节点的个数。从而达到减少整个神经网络中参数的目的
    (4)全连接层：完成最后的分类任务，卷积层和池化层已经讲图像中的信息抽象成了信息含量跟高的特征，之后仍然需要全连接层进行最后的分类
    (5)softmax层
6.3 卷积神经网络常用结构
  6.3.1 卷积层
    1.卷积层的前向传播：
      (1)卷积层是讲一个三维输入矩阵，变成一个长宽变小，深度增加的另一个三维矩阵。
      (2)并不对全部的输入矩阵进行卷积，只对一部分。
      (3)对输入矩阵使用3*3或者5*5的卷积核(其实就是一个w权重参数)进行扫描。
      (4)扫描的过程是从左上到右下，按照固定的步长，每扫描一个小矩阵，就用卷积核进行点乘(数量积)。然后将输入层的各层使用不同卷积核计算出来的结果加在一起，再加上偏置量，则就是输出节点矩阵的一个结果。
      (5)要注意，卷积层的权重参数w。是每一个输出节点都有一套w参数，每一套参数W有输入矩阵层数个的卷积核。卷积在滑动的时候，将各个输入层数据与卷积核的数量积加在一起(再加上偏置量)，就是输出矩阵对应层(通过卷积核判断对应层)的对应位置。
      (6)输出矩阵每一层并非都是1维的，输出矩阵的长宽，要通过输入矩阵的长宽和卷积核的大小通过公式计算。
    2.tensorflow实现卷积层的前向传播：
      (1)创建卷积核，和偏置量：
        filter_weight = tf.get_variable('biases',[5,5,3,16]  ----  表示输出矩阵为5*5*3,输出的深度为16
                ,initializer=tf.truncated_normal_initializer(stddev=0.1)
        biases = tf.get_variable('biases',[16]
                ,initializer=tf.constant_initializer(0.1))
      (2)前向传播：
        conv = tf.nn.conv2d(input,filter_weight,strides=[1,1,1,1],padding='SAME')
          ----  input为当前层的输入节点矩阵，input应该为一个四维矩阵，第一维表示batch中的序号，后三维为实际的数据。
          ----  filter_weight是卷积核。
          ----  strides是步长，为了对应input的维度，这里是一个有着四个元素的数组，但是因为只对x,y有步长操作，所以第一位和最后一位都必须是1(batch一个一个移动，z层一个一个移动)
          ----  padding：SAME表示全0填充，VALID表示不添加。
      (3)添加偏置量：
        bias = tf.nn.bias_add(conv,biases)  ----  为卷积层特殊准备的添加偏置量的函数，因为偏置数组中的每一个偏置量都要与一个2维的输出矩阵的每一位进行相加，所以单纯的+操作不能满足。返回添加了偏置之后的卷积输出层
      (4)最后应用激活函数：
        actived_conv = tf.nn.relu(bias)  ----  对整个真个输出结果调用激活函数即可。
  6.3.2 池化层
    1.池化层的前向传播：
      (1)也有一个与卷积核类似的过滤器，但是运算规则为取过滤器扫描中的最大值(max pooling)或者平均值(average pooling)。
      (2)池化层，也需要制定过滤器的尺寸，步长等信息，但是不同的是，不同层之间的过滤器完全一样。
      (3)其实过滤器并没有权重参数，只有过滤尺寸而已。
      (4)池化层不能减少节点矩阵的深度或者输入样例的个数。
    2.tensorflow实现池化层(最大池化层)：
      pool = tf.nn.,ax_pool(actived_conv,ksize=[1,3,3,1],strides=[1,2,2,1],padding='SAME')
        ----  actived_conv为输入节点矩阵，
        ----  为池化过滤器的尺寸大小，同样是一个4个元素的一维数组，第一位和最后一维都必须为1，意味着过滤器不能跨越不同的batch，不能跨越层数。
        ----  strides为步长，同样为1，第一位和最后一维都必须是1，因为batch上一个一个移动，深度上也是一位一位移动。
        ----  padding表示填充，'SAME'表示全零填充，'VALID'表示不填充
6.4 经典卷积网络模型
  6.4.1 LeNet-5 模型
    1.LeNet-5模型的各个层数(一共7层)：
      (1)第一层，卷积层：
        输入：32*32*1(原始数据)
        卷积核：5*5*6 VALID strides = 1
        参数：5*5*1*6+6 = 156个参数
        输出：28*28*6
        连接：28*28*6*(25+1) = 122304个连接
      (2)第二层，池化层：
        输入：28*28*6
        过滤器：2*2 strides = 2
        输出：14*14*6
      (3)第三层：卷积层：
        输入：14*14*6
        卷积核：5*5*16 VALID strides = 1
        参数：5*5*16+16 = 2416
        输出：10*10*16
        连接：10*10*16*(25+1)=41600
      (4)第四层，池化层：
        输入：10*10*16
        过滤器：2*2 strides = 2
        输出：5*5*16
      (5)第五层，全连接层：实际上是一个卷积核也是5*5的卷积层
        输入：5*5*16
        输出：120
      (6)第六层，全连接层：
        输入：120
        输出：84
        参数：120*84+8=10164
      (7)第七层，全连接层：
        输入：84
        输出：10
        参数：84*10+10 = 850

